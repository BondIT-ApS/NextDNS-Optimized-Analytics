# file: backend/models.py
from sqlalchemy import (
    create_engine,
    Column,
    Integer,
    String,
    Text,
    DateTime,
    Boolean,
    Index,
    TypeDecorator,
    UniqueConstraint,
)
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import sessionmaker
from sqlalchemy.exc import SQLAlchemyError
from datetime import datetime, timezone
import os
import json

# Set up logging
from logging_config import get_logger

logger = get_logger(__name__)


# Custom Text type that forces TEXT without JSON casting
class ForceText(TypeDecorator):
    impl = Text

    def process_bind_param(self, value, dialect):
        if value is not None:
            return str(value)
        return value

    def process_result_value(self, value, dialect):
        return value


# Base declaration
Base = declarative_base()

# Database connection setup
DATABASE_URL = f"postgresql://{os.getenv('POSTGRES_USER')}:{os.getenv('POSTGRES_PASSWORD')}@{os.getenv('POSTGRES_HOST')}/{os.getenv('POSTGRES_DB')}"
engine = create_engine(DATABASE_URL, echo=False)
Session = sessionmaker(bind=engine)


# Database model for DNS logs
class DNSLog(Base):
    __tablename__ = "dns_logs"

    id = Column(Integer, primary_key=True, autoincrement=True)
    timestamp = Column(DateTime(timezone=True), nullable=False, index=True)
    domain = Column(String(255), nullable=False, index=True)
    action = Column(
        String(50), nullable=True, default="default"
    )  # allowed, blocked, etc.
    device = Column(ForceText, nullable=True)  # Store device info as JSON string
    client_ip = Column(String(45))  # Support IPv4 and IPv6
    query_type = Column(String(10), default="A")  # A, AAAA, CNAME, etc.
    blocked = Column(Boolean, default=False, nullable=False, index=True)
    profile_id = Column(String(50), index=True)
    data = Column(ForceText, nullable=False)  # Store original raw data as JSON string
    created_at = Column(
        DateTime(timezone=True),
        default=lambda: datetime.now(timezone.utc),
        nullable=False,
    )

    # Add composite indexes and unique constraint for duplicate prevention
    __table_args__ = (
        Index("idx_dns_logs_timestamp_domain", "timestamp", "domain"),
        Index("idx_dns_logs_domain_action", "domain", "action"),
        Index("idx_dns_logs_profile_timestamp", "profile_id", "timestamp"),
        # Unique constraint to prevent duplicates based on timestamp, domain, and client_ip
        UniqueConstraint(
            "timestamp",
            "domain",
            "client_ip",
            name="uq_dns_logs_timestamp_domain_client",
        ),
    )


# Database model for tracking fetch progress
class FetchStatus(Base):
    __tablename__ = "fetch_status"

    id = Column(Integer, primary_key=True, autoincrement=True)
    last_fetch_timestamp = Column(DateTime(timezone=True), nullable=False)
    last_successful_fetch = Column(
        DateTime(timezone=True),
        default=lambda: datetime.now(timezone.utc),
        nullable=False,
    )
    records_fetched = Column(Integer, default=0, nullable=False)
    profile_id = Column(String(50), nullable=False, index=True)
    created_at = Column(
        DateTime(timezone=True),
        default=lambda: datetime.now(timezone.utc),
        nullable=False,
    )
    updated_at = Column(
        DateTime(timezone=True),
        default=lambda: datetime.now(timezone.utc),
        onupdate=lambda: datetime.now(timezone.utc),
        nullable=False,
    )

    __table_args__ = (
        # Ensure we only have one status record per profile
        UniqueConstraint("profile_id", name="uq_fetch_status_profile"),
    )


# Get total record count from database
def get_total_record_count():
    """Get the total number of DNS log records in the database.

    Returns:
        int: Total number of records, or 0 if error occurs
    """
    session = Session()
    try:
        count = session.query(DNSLog).count()
        logger.debug(f"üìä Database contains {count:,} total DNS log records")
        return count
    except SQLAlchemyError as e:
        logger.error(f"‚ùå Error getting record count: {e}")
        return 0
    finally:
        session.close()


# Initialize the database (now handled by Alembic migrations)
def init_db():
    # Note: Database initialization is now handled by Alembic migrations
    # Run 'alembic upgrade head' to initialize/update the database schema
    logger.info("üóÑ Database initialization is handled by Alembic migrations.")
    logger.info(
        "üîÑ Run 'alembic upgrade head' to ensure database schema is up to date."
    )

    # Log current database status
    total_records = get_total_record_count()
    logger.info(f"üíæ Database currently contains {total_records:,} DNS log records")


# Add log entry to the database with duplicate prevention
def add_log(log):
    """Add a DNS log entry to the database with duplicate prevention.

    Args:
        log (dict): DNS log data containing domain, action, device, and other fields

    Returns:
        tuple: (record_id, is_new) where record_id is the ID and is_new indicates if it's a new record
    """
    session = Session()
    try:
        # Extract timestamp from log data
        log_timestamp_str = log.get("timestamp")
        if log_timestamp_str:
            # Parse NextDNS timestamp format: 2025-09-18T08:11:39.673Z
            log_timestamp = datetime.fromisoformat(
                log_timestamp_str.replace("Z", "+00:00")
            )
        else:
            log_timestamp = datetime.now(timezone.utc)

        # Determine action based on NextDNS log structure
        action = log.get("action") or log.get("status") or "default"

        # Handle device - extract name if it's a dict, otherwise use as is
        device_info = log.get("device")

        # Handle client info - could be in clientIp field
        client_ip = log.get("client_ip") or log.get("clientIp")

        # Check for existing record first (duplicate prevention)
        existing_log = (
            session.query(DNSLog)
            .filter_by(
                timestamp=log_timestamp, domain=log.get("domain"), client_ip=client_ip
            )
            .first()
        )

        if existing_log:
            logger.debug(
                f"üîÑ Duplicate found for domain {log.get('domain')} at {log_timestamp} - skipping"
            )
            return existing_log.id, False  # Return existing ID, not new

        # Determine if request was blocked
        blocked = (
            log.get("blocked", False)
            or action == "blocked"
            or log.get("status") == "blocked"
        )

        # Ensure all JSON data is properly serialized as strings
        device_str = json.dumps(device_info) if device_info else None
        data_str = json.dumps(log) if isinstance(log, dict) else str(log)

        # Debug output to check data types (only in DEBUG mode)
        logger.debug(
            f"üêõ Data serialization - device type: {type(device_str)}, device value: {device_str}"
        )
        logger.debug(
            f"üêõ Data serialization - data type: {type(data_str)}, data value (first 100 chars): {str(data_str)[:100]}"
        )

        new_log = DNSLog(
            timestamp=log_timestamp,
            domain=log.get("domain"),
            action=action,
            device=device_str,
            client_ip=client_ip,
            query_type=log.get("query_type", "A"),
            blocked=blocked,
            profile_id=log.get("profile_id"),
            data=data_str,
        )
        session.add(new_log)
        session.commit()
        logger.debug(
            f"üíæ Successfully added NEW log for domain: {log.get('domain')} at {log_timestamp}"
        )
        return new_log.id, True  # Return new ID, is new
    except SQLAlchemyError as e:
        session.rollback()
        logger.error(f"‚ùå Error adding log to database: {e}")
        return None, False
    finally:
        session.close()


# Get last fetch timestamp for incremental fetching
def get_last_fetch_timestamp(profile_id):
    """Get the last successful fetch timestamp for a profile.

    Args:
        profile_id (str): NextDNS profile ID

    Returns:
        datetime: Last fetch timestamp or None if no previous fetch
    """
    session = Session()
    try:
        fetch_status = (
            session.query(FetchStatus).filter_by(profile_id=profile_id).first()
        )
        if fetch_status:
            logger.debug(
                f"üìÖ Last fetch for profile {profile_id}: {fetch_status.last_fetch_timestamp}"
            )
            return fetch_status.last_fetch_timestamp
        else:
            logger.debug(f"üìÖ No previous fetch found for profile {profile_id}")
            return None
    except SQLAlchemyError as e:
        logger.error(f"‚ùå Error getting last fetch timestamp: {e}")
        return None
    finally:
        session.close()


# Update fetch status after successful fetch
def update_fetch_status(profile_id, last_timestamp, records_count):
    """Update or create fetch status record.

    Args:
        profile_id (str): NextDNS profile ID
        last_timestamp (datetime): Timestamp of the last fetched record
        records_count (int): Number of records fetched in this batch
    """
    session = Session()
    try:
        fetch_status = (
            session.query(FetchStatus).filter_by(profile_id=profile_id).first()
        )

        if fetch_status:
            # Update existing record
            fetch_status.last_fetch_timestamp = last_timestamp
            fetch_status.last_successful_fetch = datetime.now(timezone.utc)
            fetch_status.records_fetched += records_count
            fetch_status.updated_at = datetime.now(timezone.utc)
            logger.debug(
                f"üìÖ Updated fetch status for profile {profile_id}: last_timestamp={last_timestamp}, total_records={fetch_status.records_fetched}"
            )
        else:
            # Create new record
            fetch_status = FetchStatus(
                profile_id=profile_id,
                last_fetch_timestamp=last_timestamp,
                records_fetched=records_count,
            )
            session.add(fetch_status)
            logger.debug(
                f"üìÖ Created new fetch status for profile {profile_id}: last_timestamp={last_timestamp}, records={records_count}"
            )

        session.commit()
        return True
    except SQLAlchemyError as e:
        session.rollback()
        logger.error(f"‚ùå Error updating fetch status: {e}")
        return False
    finally:
        session.close()


# Retrieve logs with optional exclusion of domains and advanced filtering
def get_logs(
    exclude_domains=None,
    search_query="",
    status_filter="all",
    profile_filter=None,
    limit=100,
    offset=0,
):
    """Retrieve DNS logs with optional filtering and pagination.

    Args:
        exclude_domains (list): List of domains to exclude from results
        search_query (str): Domain name search query
        status_filter (str): Filter by status - 'all', 'blocked', 'allowed'
        profile_filter (str): Filter by specific profile ID
        limit (int): Maximum number of records to return
        offset (int): Number of records to skip for pagination

    Returns:
        list: List of DNS log dictionaries
    """
    total_records = get_total_record_count()
    logger.debug(
        f"üìä Retrieving logs with limit={limit}, offset={offset}, exclude_domains={exclude_domains}, search='{search_query}', status='{status_filter}', profile='{profile_filter}'"
    )
    logger.info(
        f"üìä Database query: requesting {limit} records from {total_records:,} total records"
    )
    session = Session()
    try:
        query = session.query(DNSLog).order_by(DNSLog.timestamp.desc())

        # Apply domain exclusions
        if exclude_domains:
            query = query.filter(~DNSLog.domain.in_(exclude_domains))
            logger.debug(f"üö´ Excluding {len(exclude_domains)} domains from results")

        # Apply search filter on domain name
        if search_query.strip():
            query = query.filter(DNSLog.domain.ilike(f"%{search_query}%"))
            logger.debug(f"üîç Filtering by domain search: '{search_query}'")

        # Apply status filter
        if status_filter == "blocked":
            query = query.filter(DNSLog.blocked == True)
            logger.debug("üö´ Filtering for blocked requests only")
        elif status_filter == "allowed":
            query = query.filter(DNSLog.blocked == False)
            logger.debug("‚úÖ Filtering for allowed requests only")

        # Apply profile filter
        if profile_filter and profile_filter.strip():
            query = query.filter(DNSLog.profile_id == profile_filter)
            logger.debug(f"üß± Filtering for profile: '{profile_filter}'")

        # Apply pagination
        query = query.offset(offset).limit(limit)

        result = [
            {
                "id": log.id,
                "timestamp": log.timestamp.isoformat(),
                "domain": log.domain,
                "action": log.action,
                "device": (
                    json.loads(log.device)
                    if log.device and isinstance(log.device, str)
                    else log.device
                ),
                "client_ip": log.client_ip,
                "query_type": log.query_type,
                "blocked": log.blocked,
                "profile_id": log.profile_id,
                "data": (
                    json.loads(log.data)
                    if log.data and isinstance(log.data, str)
                    else log.data
                ),
                "created_at": log.created_at.isoformat(),
            }
            for log in query.all()
        ]
        logger.debug(f"üìä Retrieved {len(result)} logs from database")
        return result
    except SQLAlchemyError as e:
        logger.error(f"‚ùå Error retrieving logs from database: {e}")
        return []
    finally:
        session.close()


# Get total statistics for all logs in the database
def get_logs_stats(profile_filter=None):
    """Get statistics for DNS logs in the database, optionally filtered by profile.

    Args:
        profile_filter (str): Optional profile ID to filter statistics

    Returns:
        dict: Dictionary containing total, blocked, and allowed counts and percentages
    """
    session = Session()
    try:
        query = session.query(DNSLog)

        # Apply profile filter if specified
        if profile_filter and profile_filter.strip():
            query = query.filter(DNSLog.profile_id == profile_filter)
            logger.debug(f"üß± Getting stats for profile: '{profile_filter}'")

        # Get total count
        total_count = query.count()

        # Get blocked count
        blocked_count = query.filter(DNSLog.blocked == True).count()

        # Calculate allowed count
        allowed_count = total_count - blocked_count

        # Calculate percentages
        blocked_percentage = (
            (blocked_count / total_count * 100) if total_count > 0 else 0
        )
        allowed_percentage = (
            (allowed_count / total_count * 100) if total_count > 0 else 0
        )

        stats = {
            "total": total_count,
            "blocked": blocked_count,
            "allowed": allowed_count,
            "blocked_percentage": round(blocked_percentage, 1),
            "allowed_percentage": round(allowed_percentage, 1),
            "profile_id": profile_filter,
        }

        logger.debug(f"üìä Database stats: {stats}")
        return stats
    except SQLAlchemyError as e:
        logger.error(f"‚ùå Error getting logs statistics: {e}")
        return {
            "total": 0,
            "blocked": 0,
            "allowed": 0,
            "blocked_percentage": 0,
            "allowed_percentage": 0,
            "profile_id": profile_filter,
        }
    finally:
        session.close()


# Get list of available profiles in the database
def get_available_profiles():
    """Get list of profile IDs that have data in the database.

    Returns:
        list: List of profile IDs with record counts
    """
    session = Session()
    try:
        # Query for distinct profile IDs and their counts
        from sqlalchemy import func

        results = (
            session.query(
                DNSLog.profile_id,
                func.count(DNSLog.id).label("record_count"),
                func.max(DNSLog.timestamp).label("last_activity"),
            )
            .filter(DNSLog.profile_id.isnot(None))
            .group_by(DNSLog.profile_id)
            .order_by(func.count(DNSLog.id).desc())
            .all()
        )

        profiles = []
        for result in results:
            profiles.append(
                {
                    "profile_id": result.profile_id,
                    "record_count": result.record_count,
                    "last_activity": (
                        result.last_activity.isoformat()
                        if result.last_activity
                        else None
                    ),
                }
            )

        logger.debug(f"üß± Found {len(profiles)} profiles with data")
        return profiles
    except SQLAlchemyError as e:
        logger.error(f"‚ùå Error getting available profiles: {e}")
        return []
    finally:
        session.close()
